Model,Year,MMLU average,Training compute (petaFLOP),Training dataset size,Organization
BLOOM,2022,39.13,412000000,3.9E+11,"HuggingFace, BigScience"
BloombergGPT,2023,39.18,212000000,7.08E+11,Bloomberg
Chinchilla,2022,67.5,588000000,1.4E+12,DeepMind
GLM-130B,2022,44.8,312000000,4E+11,Tsinghua KEG
GPT-2 (finetuned),2019,32.4,36000,4000000000,OpenAI
GPT-3 (davinci),2020,43.9,393000000,3.74E+11,OpenAI
GPT-3.5,2022,70,393000000,3.74E+11,OpenAI
GPT-4,2023,86.4,20200000000,2.61E+13,OpenAI
GPT-NeoX-20B,2022,33.6,21200000,1.77E+11,Eleuther
Gopher (0.4B),2021,25.7,751000,3E+11,DeepMind
Gopher (1.4B),2021,27.3,2520000,3E+11,DeepMind
Gopher (280B),2021,60,504000000,3E+11,DeepMind
Gopher (7B),2021,29.5,12800000,3E+11,DeepMind
LLaMA (13B),2023,46.9,78000000,1E+12,Meta AI
LLaMA (33B),2023,57.8,273000000,1.4E+12,Meta AI
LLaMA (65B),2023,63.4,548000000,1.4E+12,Meta AI
LLaMA (7B),2023,35.1,40200000,1E+12,Meta AI
OPT,2022,35.99,172000000,4.34E+11,Meta AI
PaLM (540B),2022,69.3,2530000000,7.8E+11,Google Research
PaLM (62B),2022,53.7,296000000,7.95E+11,Google Research
PaLM (62B+),2022,62.8,493000000,1.33E+12,Google Research
PaLM (8B),2022,25.3,37400000,7.8E+11,Google Research
PaLM-2,2023,78.3,8160000000,4E+12,Google Research
code-davinci-002,2022,68.3,393000000,3.74E+11,OpenAI
